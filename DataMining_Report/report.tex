% Created 2013-01-28 Mon 22:41
\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{xfrac}
\geometry{a4paper, textwidth=6.5in, textheight=10in, marginparsep=7pt, marginparwidth=.6in}

\title{Data Mining 2013: Project Report}
\author{\textbf{Jared Niederhauser} - njared@student.ethz.ch\\
\textbf{Ruben Wolff} - wolffr@student.ethz.ch}
\date{\today}

\begin{document}
\maketitle

\section{Approximate retrieval - Locality Sensitive Hashing}
\begin{enumerate}
\item How was your choice of rows and bands motivated? How did you search for the
best parameters? \\ \\
\textbf{Answer}: To find suitable values for the number of bands and rows, we
started with the equation $t=(\sfrac{1}{b})^{\sfrac{1}{r}}$ where $t$ is the
desired similarity threshold, $b$ is the number of bands, and $r$ is the
number of rows.  Using a fixed similarity threshold of 80\% (as defined by the project specification), we chose initial values for
$b$ and $r$ that fell within the restriction $b*r\le 120$.  The
starting values for our LSH algorithm were $b=10$ and $r=12$.  These starting
values gave us a similarity threshold of
$(\sfrac{1}{10})^{\sfrac{1}{12}}\approx0.825$ as well as maximizing the size of
the signature matrix at 120 rows. \\ \\
After defining an initial starting point $(b=10, r=12)$, we observed what
happened when we 1) increased $b$ and decreased $r$ and 2) increased
$r$ and decreased $b$ while still keeping the product of the two
numbers below 120.  We noticed that we were receiving a much better score when
we decreased the number of bands in favor of increasing the number of rows. 
From there we confinued to decrease $b$ and increase $r$, while
keeping the product as close to 120 as possible, until we didn't see an
improvement in the score.  We observed an optimal band value at $b=5$.  Once we
found this value we continued to decrease the value of $r$, starting from the
highest value $r=24$, until no improvement in the score was noticed.  Using this
approach we received a score of $\approx 0.93$, results that were competitive
with many of the other groups.

\item Conceptually, what would you have to change if you were asked to design an image
  retrieval system that you can query for similar images given a large image
  dataset? \\

\textbf{Answer}: Code changes would depend largely on how the image shingles
are constructed and whether or not Jaccard distance is an adequate distance
metric. In the case of the project, the shingles were predefined for us, and we
were expected to use Jaccard distance as a measure of likeness.
This allowed us to use min-hashing techniques to efficiently compare the
similarities of two documents.  This was a natural approach to use for the
project as the shingles were defined using the words in the document.  If a
word was present, that word's integer ID would appear in the document's shingle
vector; however, such a representation seems poorly suited for creating image
shingles. Consequently, using Jaccard distance as a means of determining the
likeness of images could be ill-suited. \\ \\
Even if Jaccard distance won't work well when comparing image shingles, there
are several other distance metrics that could be well suited and that would
still provide a locality sensitive hash family that is 
$(d_1,d_2,p_1,p_2)-sensitive$. If such a distance function is sufficient for
comparing images (e.g. cosine distance, Euclidean distance, etc.), very few changes 
to the code would need to be made. These distance functions can still utilize 
the and-or construction and thus the band/rows technique wouldn't need to be
altered.  The only modifications necessary would be that of changing the hashing
function to match the desired distance metric, and also selecting suitable
values for the number of bands/rows such that the number of false
positives/negatives are minimized.


\end{enumerate}

\section{Large-scale Supervised Learning}

\begin{enumerate}
\item Which algorithms did you consider? Which one did you choose for the
  final submission and why? \\ \\
\textbf{Answer}: \emph{Ruben to start}

\item How did you select the parameters for your model? Which are the
  most important parameters of your model? \\ \\
\textbf{Answer}: \emph{Ruben to start}

\end{enumerate}

\section{Recommender Systems}

\begin{enumerate}
\item Which algorithm did you implement? What was your motivation? \\ \\
\textbf{Answer}: Our motivation in this project was to start with a very simple algorithm,
measure its performance, and then improve upon it with more complex models.  In
the end, we implemented 4 different algorithms: UCB1, LinUCB-Disjoint, LinUCB-Hybrid, and an SVM
based approach.  In the end we acheived the best results with LinUCB-Disjoint,
however we had very good results with the LinUCB-Hybrid and SVM based approaches
as well.

\item How did you select the parameters for your model? \\ \\
\textbf{Answer}: For the LinUCB-Disjoint model, there is only one parameter
$\alpha$ that controls the tradeoff between expected reward and our confidence,
i.e. the tradeoff between exploring articles we aren't sure about and exploiting
high performing articles. We chose our initial alpha value based on the equation
$\alpha=1+\sqrt{\ln(2/\delta)/2}$ with $\delta=0.05$.  This gave us the value
$\alpha\approx2.358$ which we refined through trial and error until we settled
on the final value of $0.835$. \\ \\
Additionally, we had an internal parameter for our code.  Because it was too
inefficient to do matrix inversion every time the \textit{updatePolicy()}
function was called, we had a parameter that controlled how many times an
article, $a_i$, must be chosen before its matrix inverse, $A_{i}^{-1}$, was
recalculated. We wanted to keep this value as low as possible without running
over the designated time limit so that we would get good speed without
negatively impacting the CTR. We found that setting this value to 5 (i.e.
$A_{i}^{-1}$ would be updated every $5^{th}$ time the $i^{th}$ article was
chosen) gave us a good balance between speed and performance.

\item Does the performance measured in CTR increase monotonically during the
execution of your algorithm? Why? \\ \\
\textbf{Answer}: No, the CTR does not increase monotonically during the
algorithm's execution.  The inherent nature of these explore-exploit algorithms
is that options with suboptimal previous performance will be chosen in an effort
to increase our confidence in their expected return.  When the algorithm first
starts, all choices are unknown and thus the CTR increases/decreases solely by
chance.  However, as we collect more results from each of the articles we become
more confident that our empirical model approaches the true model.  As a result,
there may be steps during the algorithm's execution where the intermediate CTR
is higher than the CTR achieved by the final learned model.  A trivial example
of this would be the algorithm's CTR after a single iteration.  If the
algorithm, by chance alone, manages to correctly choose an article that was
clicked, the CTR will be at a perfect $1.0$.  This, however, is a meaningless
CTR that is not indicative of the final CTR and will surely decrease over time.

\end{enumerate}

\end{document}