% Created 2013-01-28 Mon 22:41
\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{xfrac}
\geometry{a4paper, textwidth=6.5in, textheight=10in, marginparsep=7pt, marginparwidth=.6in}

\title{Data Mining 2013: Project Report}
\author{\textbf{Jared Niederhauser} - njared@student.ethz.ch\\
\textbf{Ruben Wolff} - wolffr@student.ethz.ch}
\date{\today}

\begin{document}
\maketitle

\section{Approximate retrieval - Locality Sensitive Hashing}
\begin{enumerate}
\item How was your choice of rows and bands motivated? How did you search for the
best parameters? \\ \\
\textbf{Answer}: To find suitable values for the number of bands and rows, we
started with the equation $t=(\sfrac{1}{b})^{\sfrac{1}{r}}$ where $t$ is the
desired similarity threshold, $b$ is the number of bands, and $r$ is the
number of rows.  Using a fixed similarity threshold of 80\% (as defined by the project specification), we chose initial values for
$b$ and $r$ that fell within the restriction $b*r\le 120$.  The
starting values for our LSH algorithm were $b=10$ and $r=12$.  These starting
values gave us a similarity threshold of
$(\sfrac{1}{10})^{\sfrac{1}{12}}\approx0.825$ as well as maximizing the size of
the signature matrix at 120 rows. \\ \\
After defining an initial starting point $(b=10, r=12)$, we observed what
happened when we 1) increased $b$ and decreased $r$ and 2) increased
$r$ and decreased $b$ while still keeping the product of the two
numbers below 120.  We noticed that we were receiving a much better score when
we decreased the number of bands in favor of increasing the number of rows. 
From there we continued to decrease $b$ and increase $r$, while
keeping the product as close to 120 as possible, until we didn't see an
improvement in the score.  We observed an optimal band value at $b=5$.  Once we
found this value we continued to decrease the value of $r$, starting from the
highest value $r=24$, until no improvement in the score was noticed.  Using this
approach we received a score of $\approx 0.93$, results that were competitive
with many of the other groups.

\item Conceptually, what would you have to change if you were asked to design an image
  retrieval system that you can query for similar images given a large image
  dataset? \\

\textbf{Answer}: Code changes would depend largely on how the image shingles
are constructed and whether or not Jaccard distance is an adequate distance
metric. In the case of the project, the shingles were predefined for us, and we
were expected to use Jaccard distance as a measure of likeness.
This allowed us to use min-hashing techniques to efficiently compare the
similarities of two documents.  This was a natural approach to use for the
project as the shingles were defined using the words in the document.  If a
word was present, that word's integer ID would appear in the document's shingle
vector; however, such a representation seems poorly suited for creating image
shingles. Consequently, using Jaccard distance as a means of determining the
likeness of images could be ill-suited. \\ \\
Even if Jaccard distance won't work well when comparing image shingles, there
are several other distance metrics that could be well suited and that would
still provide a locality sensitive hash family that is 
$(d_1,d_2,p_1,p_2)-sensitive$. If such a distance function is sufficient for
comparing images (e.g. cosine distance, Euclidean distance, etc.), very few changes 
to the code would need to be made. These distance functions can still utilize 
the and-or construction and thus the band/rows technique wouldn't need to be
altered.  The only modifications necessary would be that of changing the hashing
function to match the desired distance metric, and also selecting suitable
values for the number of bands/rows such that the number of false
positives/negatives are minimized.


\end{enumerate}

\section{Large-scale Supervised Learning}

\begin{enumerate}
\item Which algorithms did you consider? Which one did you choose for the
  final submission and why? \\ \\
\textbf{Answer}:  
We considered an SVM solution directly but it needed to be approximated due to
the scale of the input data. Another possibility would have been to take the
entire dataset as a matrix where users are rows and columns are all possible
stories they could have clicked on, with some preprocessing this matrix could
have been transformed using SVD and cutting out small singular values. The
dataset did not lend itself well for the SVD solution because each user gives
very few datapoints and with this method we are not using the feature
information of the users. It would have been also possible to fit a regression
curve to an augmented [user, article] vector with the Y-axis simply being
whether or not the user clicked the article. But this approach is better suited
for data where the user gives a rating not a simple click or no-click.
\\ \\
We chose Online Support Vector Machine(SVM). SVMs have good theoretical
guarantees but, since in this project we had very large training set, the run
time of the quadratic programming needed for standard SVMs were not acceptable.
Online convex programming can be used to approximate the optimal decision
boundary after observing each datapoint individually. The regret of Online
Convex Programming for a given loss function $l$ and the function to be
optimized $f$. In the case of the online SVM $f$ is a LaGrange transformation
of $||w||$ with constraint that all points $x_i$ are classified correctly
using $w^T x_i + b$. The regret of Online convex programming is bounded by $R_T
\leq \frac{||S||^2 \sqrt{T}}{2} + (\sqrt{T} - \frac{1}{2}) max_t(||\nabla
f_t||^2)$. Where $S$ is the feasible set of solutions and $T$ is the number of
steps or datapoints we have. An additional benefit of Online SVM is that it can
be easily divided among machines in a MapReduce fashion.

\item How did you select the parameters for your model? Which are the
  most important parameters of your model? \\ \\
\textbf{Answer}: The model parameters were chosen by sampling from the
possible parameter space and then doing a short gradient descent starting from
the best sample. The parameters were compared based on 5 fold cross validation.
The most important parameters for the parallel gradient descent are $\lambda$
and $\eta$. $\lambda$ is the constraint on the norm of the normal vector of the
decision plane $w$, as in $S=(w:||w||\leq \frac{1}{\lambda})$.and $\eta$ can be
considered a step size for changing $w$ as in $w_{t+1} = Proj_S(w_t - \eta
\nabla f_t(w_t))$.

\end{enumerate}

\section{Recommender Systems}

\begin{enumerate}
\item Which algorithm did you implement? What was your motivation? \\ \\
\textbf{Answer}: Our motivation in this project was to start with a very simple algorithm,
measure its performance, and then improve upon it with more complex models.  In
the end, we implemented 4 different algorithms: UCB1, LinUCB-Disjoint,
LinUCB-Hybrid, and an SVM based approach.  The first and simplest algorithm we
implemented was UCB1.  Because it is such a simple, context-free model, it
didn't yield very good performance, but it did provide us a good starting point
for improvement in implementing the more complex algorithms. In the end we
achieved the best results with LinUCB-Disjoint, however we had comparable
results with the LinUCB-Hybrid and our SVM based approaches as well.

\item How did you select the parameters for your model? \\ \\
\textbf{Answer}: For the LinUCB-Disjoint model, there is only one parameter
$\alpha$ that controls the tradeoff between expected reward and our confidence,
i.e. the tradeoff between exploring articles we aren't sure about and exploiting
high performing articles. We chose our initial alpha value based on the equation
$\alpha=1+\sqrt{\ln(2/\delta)/2}$ with $\delta=0.05$.  This gave us the value
$\alpha\approx2.358$ which we refined through trial and error until we settled
on the final value of $0.835$. \\ \\
Additionally, we had an internal parameter for our code.  Because it was too
inefficient to do matrix inversion every time the \textit{updatePolicy()}
function was called, we had a parameter that controlled how many times an
article, $a_i$, must be chosen before its matrix inverse, $A_{i}^{-1}$, was
recalculated. We wanted to keep this value as low as possible without running
over the designated time limit so that we would get good speed without
negatively impacting the CTR. We found that setting this value to 5 (i.e.
$A_{i}^{-1}$ would be updated every $5^{th}$ time the $i^{th}$ article was
chosen) gave us a good balance between speed and performance.

\item Does the performance measured in CTR increase monotonically during the
execution of your algorithm? Why? \\ \\
\textbf{Answer}: No, the CTR does not increase monotonically during the
algorithm's execution.  The inherent nature of these explore-exploit algorithms
is that options with suboptimal previous performance will be chosen in an effort
to increase our confidence in their expected return.  When the algorithm first
starts, all choices are unknown and thus the CTR increases/decreases solely by
chance.  However, as we collect more results from each of the articles we become
more confident that our empirical model approaches the true model.  As a result,
there may be steps during the algorithm's execution where the intermediate CTR
is higher than the CTR achieved by the final learned model.  A trivial example
of this would be the algorithm's CTR after a single iteration.  If the
algorithm, by chance alone, manages to correctly choose an article that was
clicked, the CTR will be at a perfect $1.0$.  This, however, is a meaningless
CTR that is not indicative of the final CTR and will surely decrease over time.

\end{enumerate}

\end{document}